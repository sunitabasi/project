# -*- coding: utf-8 -*-
"""salary prediction regression project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-UL5GdNWvLMor5OytdpwRGrJKMiHMs1k
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# %matplotlib inline
from matplotlib.pyplot import xticks
from matplotlib.pyplot import yticks

data_df = pd.DataFrame(pd.read_csv('Salary Datas.csv'))
data_df

row,col = data.shape
print("no of rows =", row)
print("no of col =", col)

data_df.describe()

data_df.info()

data_df.isna().sum()

data_df = data_df.dropna()

data_df.info()

data_df.columns

data_df.shape

# calculating length of the data
print(len(set(data_df['Gender'])))
print(len(set(data_df['Education Level'])))
print(len(set(data_df['Job Title'])))

# plotting the data using seaborn pairplot which shows the relationship between the two variables
sns.pairplot(data_df)

auto = data_df[['Age','Salary','Years of Experience']]  # only taking categorical value for correlation coefficient
auto.corr()

# plotting the data
plt.figure(figsize=(15,5))
sns.heatmap(auto.corr(),annot = True)
plt.show()

auto = data_df[['Age','Gender','Education Level','Years of Experience','Salary']]
auto

#dimendion reduction by assigning boolean values
gender = pd.get_dummies(auto['Gender'],drop_first = True)
gender

auto= pd.concat([auto,gender], axis = 1)
auto

education = pd.get_dummies(auto['Education Level'], drop_first = False)
education

auto = pd.concat([auto,education],axis = 1)
auto

auto= auto.drop(['Gender','Education Level'],axis = 1)
auto

auto.corr()

plt.figure(figsize =(15,5)) # this shows the correlation coefficient between two variables
sns.heatmap(auto.corr(),annot= True)
plt.show()

from sklearn.model_selection import train_test_split
# We specify this so that the train and test data set always have the same rows, respectively
df_train, df_test = train_test_split(auto, train_size = 0.85, test_size = 0.15, random_state = 1)

df_train.shape

df_test.shape

from sklearn.linear_model import LinearRegression
X_train = df_train[['Age','Years of Experience','Male',"Bachelor's","Master's",'PhD']]
y_train = df_train['Salary'].astype('int')


lm = LinearRegression()
lr_model = lm.fit(X_train, y_train)

# # #for logistic regression
from sklearn.linear_model import LogisticRegression
lg = LogisticRegression()
lg_model = lg.fit(X_train, y_train)

# # #for polynomial regression
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=7)
X_poly = poly_reg.fit_transform(X_train)
pol_reg = LinearRegression()
pol_reg.fit(X_poly, y_train)

#training acc
print("Linear regression = ",lr_model.score(X_train, y_train)*100)
print("Logistic regression = ",lg_model.score(X_train, y_train)*100)
print("Polynomial regression = ",pol_reg.score(poly_reg.fit_transform(X_train), y_train)*100)

X_test = df_test
y_test = auto['Salary'].astype('int')
X_test = auto.drop(['Salary'],axis = 1)

X_test

y_test

#test acc
print("Linear regression = ",lr_model.score(X_test, y_test)*100)
print("Logistic regression = ",lg_model.score(X_test, y_test)*100)
print("Polynomial regression = ",pol_reg.score(poly_reg.fit_transform(X_test), y_test)*100)

data = df_test.iloc[21:22].astype('int')  # select the specific row to test the data
data

actual_salary = data['Salary']
data = data.drop(['Salary'],axis = 1)
data

print("Predicted Salary using linear regression",lr_model.predict(data))
print("Predicted Salary using logistic regression",lg_model.predict(data))
print("Predicted Salary using Polynomial regression",pol_reg.predict(poly_reg.fit_transform(data)))

print("Actual Salary",actual_salary)
# according to the analysis it seems that linear regression analysis prediction was near to the actual salary

actual_salary

data = {'Age':[int(input("Enter age = ")) for i in range(1)],
        'Years of Experience':[float(input("Enter years of Experience = ")) for i in range(1)],
        'Male':[int(input("Gender? Enter 1 or 0 =")) for i in range(1)],
        "Bachelor's":[int(input("Bachelors's degree ? Enter 1 or 0 = ")) for i in range(1)],
        "Master's":[int(input("Master's degree ? Enter 1 or 0 = ")) for i in range(1)],
         "PhD":[int(input("Phd degree ? Enter 1 or 0 = ")) for i in range(1)]}

df = pd.DataFrame(data)
predicted_salary = lr_model.predict(df)

print("predicted Salary",predicted_salary)



df

plt.scatter(y_train,lr_model.predict(X_train),color = 'red')  # plotting the data in scatterplot
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.show()

plt.scatter(y_train,lg_model.predict(X_train),color = 'red')
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.show()

plt.scatter(y_train,pol_reg.predict(poly_reg.fit_transform(X_train)),color = 'red')
plt.xlabel('Actual Salary')
plt.ylabel('Predicted Salary')
plt.show()

# from this regression analysis of salary prediction we can predict the trend between actual and predicted salary of a person on the basis of age,years of experience,gender and education level.