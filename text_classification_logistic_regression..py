# -*- coding: utf-8 -*-
"""text_classification_logistic_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mEqzHOhVLBQ6u8LxQktzbkg7QIAHSaru
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import re,string
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
import pickle
import nltk
nltk.download('stopwords')
# %matplotlib inline



df = pd.read_csv('/content/bbc-text.csv', encoding = 'latin1')
df = df.sample(frac = 1)
df

#No of a categories
set(df['category'])

df.groupby('category').category.count()

#Analyzing data
df.groupby('category').category.count().plot.bar()
plt.show()

# Data Cleaning using regex
regs = re.sub("[^a-zA-Z]", " ", df['text'][0]).lower()
regs

#stop words
nltk.download('stopwords')
words = stopwords.words("english")
print(words)

# Data Cleaning using stemmer

stemmer = PorterStemmer()
data = "I am loving computing I have a computer".split()
" ".join([stemmer.stem(i) for i in data])
# stemmer.stem("")

print(words)
news = df['text'][0].split()
for i in words:
  c = news.count(i)
  for j in range(c):
    news.remove(i)

print(" ".join(news))

" ".join([i for i in regs.lower().split() if i not in words])

# Data Cleaning removing stopwords
words = stopwords.words("english")
without_stop_words_of_a_news = " ".join([i for i in regs.lower().split() if i not in words])
without_stop_words_of_a_news

df = df.dropna()

# Doing all cleaning process using regex, stemmer, stopwords for all data
# df['cleaned'] = list(filter(lambda x: [i for i in re.sub("[^a-zA-Z]", " ", x) ],df['text']))
df['cleaned'] = df['text'].apply(lambda x: [i for i in re.sub("[^a-zA-Z]", " ", x)])

df['cleaned'] = df['text'].apply(lambda x: " ".join([stemmer.stem(i) for i in x.lower().split()]))
df

# " ".join([stemmer.stem(i) for i in without_stop_words_of_a_news.lower().split()])

# list(filter(lambda x: [stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", x).split() if i not in words],df['text']))

# #cleaning dataset
# nltk.download('stopwords')
# stemmer = PorterStemmer()
# words = stopwords.words("english")
# words.extend(['a','an','the'])
# df['cleaned'] = df['cleaned'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", x.lower()).split() if i not in words]).lower())
# # df['newcleaned'] = [(i for i in list(df['cleaned'])).split() if i not in words ]
# df

df.to_csv('cleaned_news.csv')

# df['cleaned'] = df['text'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", x).split() ]).lower())
# df

# print(words)

# words = stopwords.words("nepali")
# words

df['cleaned'] = df['text'].apply(lambda x: " ".join([stemmer.stem(i) for i in re.sub("[^a-zA-Z]", " ", x).split() if i not in words]).lower())
df

